@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{lopezpaz2016unifying,
      title={Unifying distillation and privileged information}, 
      author={David Lopez-Paz and Léon Bottou and Bernhard Schölkopf and Vladimir Vapnik},
      year={2016},
      eprint={1511.03643},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{zhuang2019acomprehensive,
  author    = {Fuzhen Zhuang and
               Zhiyuan Qi and
               Keyu Duan and
               Dongbo Xi and
               Yongchun Zhu and
               Hengshu Zhu and
               Hui Xiong and
               Qing He},
  title     = {A Comprehensive Survey on Transfer Learning},
  journal   = {CoRR},
  volume    = {abs/1911.02685},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02685},
  eprinttype = {arXiv},
  eprint    = {1911.02685},
  timestamp = {Sat, 29 Aug 2020 18:19:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02685.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{glorot2010understanding,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@INPROCEEDINGS{yu2017oncompressing,
  author={Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={On Compressing Deep Models by Low Rank and Sparse Decomposition}, 
  year={2017},
  volume={},
  number={},
  pages={67-76},
  doi={10.1109/CVPR.2017.15}
}

@online{fashionmnist,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@misc{adam,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{net2net,
  doi = {10.48550/ARXIV.1511.05641},
  
  url = {https://arxiv.org/abs/1511.05641},
  
  author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Net2Net: Accelerating Learning via Knowledge Transfer},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{chen2020stabilizing,
  title={Stabilizing differentiable architecture search via perturbation-based regularization},
  author={Chen, Xiangning and Hsieh, Cho-Jui},
  booktitle={International conference on machine learning},
  pages={1554--1565},
  year={2020},
  organization={PMLR}
}
@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE international conference on big data (Big data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}


@article{Koturwar2017WeightIO,
  title={Weight Initialization of Deep Neural Networks(DNNs) using Data Statistics},
  author={Saiprasad Koturwar and Shabbir Hussain I Merchant},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.10570}
}


@InProceedings{pmlr-v157-skorski21a,
  title = 	 {Revisiting Weight Initialization of Deep Neural Networks},
  author =       {Skorski, Maciej and Temperoni, Alessandro and Theobald, Martin},
  booktitle = 	 {Proceedings of The 13th Asian Conference on Machine Learning},
  pages = 	 {1192--1207},
  year = 	 {2021},
  editor = 	 {Balasubramanian, Vineeth N. and Tsang, Ivor},
  volume = 	 {157},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--19 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v157/skorski21a/skorski21a.pdf},
  url = 	 {https://proceedings.mlr.press/v157/skorski21a.html},
  abstract = 	 {The proper {\em initialization of weights} is crucial for the effective training and fast convergence of {\em deep neural networks} (DNNs). Prior work in this area has mostly focused on the principle of {\em balancing the variance among weights per layer} to maintain stability of (i) the input data propagated forwards through the network, and (ii) the loss gradients propagated backwards, respectively. This prevalent heuristic is however agnostic of dependencies among gradients across the various layers and captures only first-order effects per layer. In this paper, we investigate a {\em unifying approach}, based on approximating and controlling the {\em norm of the layers’ Hessians}, which both generalizes and explains existing initialization schemes such as {\em smooth activation functions}, {\em Dropouts}, and {\em ReLU}.}
}

@inproceedings{BucilaCN06,
  added-at = {2018-11-30T00:00:00.000+0100},
  author = {Bucila, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle = {KDD},
  editor = {Eliassi-Rad, Tina and Ungar, Lyle H. and Craven, Mark and Gunopulos, Dimitrios},
  isbn = {1-59593-339-5},
  pages = {535-541},
  publisher = {ACM},
  title = {Model compression.},
  year = 2006
}

@InProceedings{Dietterich2000,
author="Dietterich, Thomas G.",
title="Ensemble Methods in Machine Learning",
booktitle="Multiple Classifier Systems",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--15",
abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
isbn="978-3-540-45014-6"
}

@article{Zheng2020,
    title = {Adversarial Attacks and Defenses in Deep Learning},
    journal = {Engineering},
    volume = {6},
    number = {3},
    pages = {346--360},
    year = {2020},
    author = {Ren, K. and Zheng, T. and Qin, Z. and Liu, X.},
}

@article{Han2020,
	author={Xu, H. and Ma, Y. and Liu, H. and Deb, D. and Liu, H. and Tang,J. and Jain, A.},
	Journal = {International Journal of Automation and Computing},
	Number = {2},
	Pages = {151--178},
	Title = {Adversarial Attacks and Defenses in Images, Graphs and Text: A Review},
	Volume = {17},
	Year = {2020},
}

@inproceedings{akiba2019optuna,
  title={Optuna: A next-generation hyperparameter optimization framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2623--2631},
  year={2019}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{cifar10,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@article{bai1996some,
  title={Some large-scale matrix computation problems},
  author={Bai, Zhaojun and Fahey, Gark and Golub, Gene},
  journal={Journal of Computational and Applied Mathematics},
  volume={74},
  number={1-2},
  pages={71--89},
  year={1996},
  publisher={Elsevier}
}

@inproceedings{sun2019meta,
  title={Meta-transfer learning for few-shot learning},
  author={Sun, Qianru and Liu, Yaoyao and Chua, Tat-Seng and Schiele, Bernt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={403--412},
  year={2019}
}

@article{shamir2020anti,
  title={Anti-distillation: Improving reproducibility of deep networks},
  author={Shamir, Gil I and Coviello, Lorenzo},
  journal={arXiv preprint arXiv:2010.09923},
  year={2020}
}
